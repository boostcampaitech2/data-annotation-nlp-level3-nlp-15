{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# designate root path for the data\n",
    "DATA_ROOT_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "list_files = glob(os.path.join(DATA_ROOT_PATH, \"*.txt\"))\n",
    "len(list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Kss...\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "# read txt file from line by line\n",
    "def read_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "# make preprocessing function\n",
    "def preprocess(list_lines:list) -> list:\n",
    "    # remove \\n\n",
    "    list_lines = [line.strip() for line in list_lines]\n",
    "    \n",
    "    # remove empty lines\n",
    "    list_lines = [line for line in list_lines if line]\n",
    "    \n",
    "    # remove duplicate elements from the list_lines\n",
    "    list_lines = list(set(list_lines))\n",
    "    return list_lines\n",
    "\n",
    "\n",
    "def break_sentence(list_sentences:list) -> list:\n",
    "    \"\"\" \n",
    "    break the string items of the list_poetic_sentences by \".\", \"!\", \"?\" \n",
    "    into two different string and and back to the list \n",
    "    \"\"\"\n",
    "    \n",
    "    # drop empty items\n",
    "    list_sentences = [sentence for sentence in list_sentences if sentence]\n",
    "\n",
    "    # strip whitespace\n",
    "    list_sentences = [sentence.strip() for sentence in list_sentences]\n",
    "\n",
    "    # split string item into sublist\n",
    "    list_sentences = [kss.split_sentences(sentence) for sentence in list_sentences]\n",
    "\n",
    "    # pull out items from the nested list\n",
    "    list_sentences = [item for sublist in list_sentences for item in sublist]\n",
    "\n",
    "    # drop empty items\n",
    "    list_sentences = [sentence for sentence in list_sentences if sentence]\n",
    "\n",
    "    return list_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# make sampling function from the list\n",
    "def sampling(list_lines:list, n:int) -> list:\n",
    "    # sampling\n",
    "    list_lines = np.random.choice(list_lines, n)\n",
    "    list_lines = list(list_lines)\n",
    "    return list_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/마이크로소프트.txt',\n",
       " './data/코세라.txt',\n",
       " './data/인텔.txt',\n",
       " './data/IBM.txt',\n",
       " './data/징동닷컴.txt',\n",
       " './data/페이팔.txt',\n",
       " './data/트위치.txt',\n",
       " './data/핀터레스트.txt',\n",
       " './data/어도비 (기업).txt',\n",
       " './data/이베이.txt',\n",
       " './data/링크드인.txt',\n",
       " './data/VM웨어.txt',\n",
       " './data/오큘러스 VR.txt',\n",
       " './data/구글 검색.txt',\n",
       " './data/구글.txt',\n",
       " './data/제네시스 (기업).txt',\n",
       " './data/스포티파이.txt',\n",
       " './data/야후! 재팬.txt',\n",
       " './data/페이스북.txt',\n",
       " './data/닌텐도.txt',\n",
       " './data/트위터.txt',\n",
       " './data/유튜브.txt',\n",
       " './data/오라클 (기업).txt',\n",
       " './data/알리바바 그룹.txt',\n",
       " './data/드롭박스.txt',\n",
       " './data/틱톡.txt',\n",
       " './data/아마존 (기업).txt',\n",
       " './data/ARM 홀딩스.txt',\n",
       " './data/텀블러 (마이크로블로그).txt',\n",
       " './data/레드햇.txt',\n",
       " './data/바이두.txt',\n",
       " './data/넷플릭스.txt',\n",
       " './data/엔비디아.txt',\n",
       " './data/아이치이.txt',\n",
       " './data/널소프트.txt',\n",
       " './data/알파벳 (기업).txt',\n",
       " './data/인스타그램.txt',\n",
       " './data/익스피디아 그룹.txt',\n",
       " './data/애플.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = [\n",
    "    './data/텀블러 (마이크로블로그).txt',\n",
    "    # './data/레드햇.txt', # IndexError: list index out of range\n",
    "    './data/바이두.txt',\n",
    "    './data/넷플릭스.txt',\n",
    "    './data/엔비디아.txt',\n",
    "    './data/아이치이.txt',\n",
    "    './data/널소프트.txt',\n",
    "    './data/알파벳 (기업).txt',\n",
    "    './data/인스타그램.txt',\n",
    "    './data/익스피디아 그룹.txt',\n",
    "    './data/애플.txt'\n",
    "]\n",
    "\n",
    "list_concat = []\n",
    "\n",
    "for file_item in list_files:\n",
    "    lines = read_txt(file_item)\n",
    "    lines = preprocess(lines)\n",
    "    lines = break_sentence(lines)\n",
    "    list_concat = list_concat + lines\n",
    "\n",
    "print(len(list_concat))\n",
    "sampling(list_concat, n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2339 880\n"
     ]
    }
   ],
   "source": [
    "# read list from the result_until_arm.txt\n",
    "list_concat_loaded = []\n",
    "with open('./result_until_arm.txt', 'r', encoding='utf-8') as f:\n",
    "    list_concat_loaded = f.readlines()\n",
    "print(len(list_concat_loaded), len(list_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3219"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_concat_combined = list_concat_loaded + list_concat\n",
    "len(list_concat_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"틱톡은 2020년 연말 세계 각 국에서 글로벌 캠페인 <Year On TikTok>을 진행한다. 2020년 틱톡 트렌드 발표, 엔터테인먼트 발표와 더불어 'Year on TikTok' 인앱 기능을 출시하여 모든 사용자가 각자의 개인적인 추억을 되돌아볼 수 있는 기회를 제공한다. <Year On TikTok> 영상을 통해 사용자는 2020년 한 해 동안 가장 좋아한 음원, 편집 효과, 콘텐츠 장르 등을 종합한 자신들의 2020년 하이라이트를 돌아본다.\\n\",\n",
       " \"인텔은 2016년 7월 '클라우드'와 '사물인터넷' 진입으로 초연결 시대를 선언하였다.\\n\",\n",
       " '그러나, 스티브 잡스가 CEO로 애플에 복귀한 다음 1998년 선보인 아이맥은 다시금 애플의 컴퓨터가 일반 소비자시장에서 인기를 끌게 되는 계기가 되었고, 이러한 아이맥의 성공에 따라 애플은 위기에서 벗어나게 된다.',\n",
       " '서비스에 포함된 게임은 에이펙스 레전드, 레전드 오브 룬테라, 피파 얼티밋 팀, 전략적 팀 전투, 모바일 레전드: 뱅뱅, 둠 이터널 등이었다.\\n',\n",
       " '검색어의 의미를 이해하기 위해 단어를 분석하기, 검색어와 일치하는 정보가 포함된 웹페이지를 검색하기, 페이지의 유용성을 평가하여 순위를 매기기, 사용자의 위치나 이전 검색 기록과 같은 맥락을 고려하여 사용자에게 알맞은 검색 결과를 제공하기, 검색 결과가 사용자의 검색 유형에 유용한지 고려하여 최상의 결과를 제공한다.\\n',\n",
       " '캐시 크기는 적어도 1GB 이상을 권장한다.\\n',\n",
       " '틱톡은 인도네시아와 방글라데시 등 국가별로 간헐적으로 차단되어 왔다.\\n',\n",
       " \"알리바바그룹의 한국 파트너사인 '이상글로벌'은 지난 2009년부터 3년간의 적자에도 불구하고 계속적으로 파트너쉽을 맺고 한국 내에서의 사업을 진행해왔으나, 알리바바 측에서 계약을 일방적으로 해지해 논란이 일었다.\\n\",\n",
       " '2015년 6월 11일 북한 고려호텔에서 발생한 화재 사건 며칠 뒤 당국은 사건 사진이 퍼지지 않도록 인스타그램 차단에 나섰다.',\n",
       " '대한민국에서는 2012년에 법인을 설립한 이후, 글로벌셀링 사업과 클라우드 사업을 진행해 오고 있다.\\n',\n",
       " '이전까지는 그러한 캐릭터 IP를 게임외에서 이용하는 것에 소극적인 입장을 취하고 있었지만, 2014년 이후는 그 방침을 전환해 적극적으로 활용하게 되었다[78].\\n',\n",
       " '(영어) 널소프트 - 공식 웹사이트',\n",
       " '아이폰은 이후 수차례의 업그레이드를 실시하여 현재(2020년 10월) 기준으로 가장 최신 모델은 아이폰 12, 아이폰 12 미니, 아이폰 12 프로, 아이폰 12 프로 맥스, 그리고 아이폰 SE 2세대가 있다.',\n",
       " '이베이는 단순한 수집품을 넘어서 판매 가능한 거의 모든 물건들로 제품 분류를 확대함에 따라 비즈니스는 더욱 빠르게 성장하였다.\\n',\n",
       " '전 세계 인구 3명 가운데 한 사람이 페이스북을 한다고 할 수 있다.\"Compete.com\"에 따르면 페이스북은 활동 사용자들 가운데 가장 많이 이용되는 소셜 네트워크 서비스라고 한다.\\n',\n",
       " '트위터 자체에도 검색 기능이 있지만, 기능이 미약하여 정보를 파악하기 힘든 점 때문에, 많은 트위터용 검색 엔진들이 만들어지고 있다.\\n',\n",
       " '페이스북은 다양한 방법으로 사회적인 삶과 사람들의 활동에 영향을 미쳤다.\\n',\n",
       " '2011년 7월, 트위치는 2015년 8월 기준으로 11,000명 이상의 회원을 확보한 파트너 프로그램을 시작했다.\\n',\n",
       " '그러나 최근 스티브 잡스의 공식 전기가 출판되면서 수록된 롭 야노프가 처음 애플의 로고 제작을 의뢰받고 만든 2가지 도안(사과를 한 입 베어문 것과 베어물지 않은 것) 중 한 입 베어물지 않은 것은 체리인지 사과인지 구분할 수 없어 투표를 통해 현재의 한 입 베어문 사과 로고가 탄생되었다는 설이 가장 유력하다.',\n",
       " '이 앱은 출시되자마자 자동으로 동영상 재생을 시작하는데, 케빈 시스트롬 CEO는 이 기능이 동영상을 찾아야 하는 다른 비디오 플랫폼와는 대조적이라고 말했다.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_individual_sentences(input_list):\n",
    "    # drop items length less than 15 characters\n",
    "    input_list_dropped = [item for item in input_list if len(item) > 15]\n",
    "\n",
    "    # drop items where the string is only constituted with alphabets\n",
    "    # input_list_dropped = [item for item in input_list_dropped if not item.isalpha()]\n",
    "    return input_list_dropped\n",
    "\n",
    "list_concat_dropped = preprocess_individual_sentences(list_concat_combined)\n",
    "print(len(list_concat_dropped))\n",
    "sampling(list_concat_dropped, n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list of sentences to a txt file\n",
    "with open(os.path.join(\"./\", \"result.txt\"), 'w', encoding='utf-8') as f:\n",
    "    for sentence in list_concat_dropped:\n",
    "        if \"\\n\" in sentence:\n",
    "            f.write(sentence)\n",
    "        else:\n",
    "            f.write(sentence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2505"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./result.txt', 'r', encoding='utf-8') as f:\n",
    "    list_concat_loaded = f.readlines()\n",
    "len(list_concat_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2505/2505 [00:00<00:00, 6336.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# make individual files as sentences\n",
    "from tqdm import tqdm\n",
    "\n",
    "for index_no, item in enumerate(tqdm(list_concat_loaded)):\n",
    "    # name the file as result_0001.txt, result_0002.txt, etc.\n",
    "    with open(os.path.join(\"./\", \"result_\" + str(index_no).zfill(4) + \".txt\"), 'w', encoding='utf-8') as f:\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86cc1fc39ab6acfc9b98d188dcc233e58c381403d84a60c8a993907295df4c51"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('aitech': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
